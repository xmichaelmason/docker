{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b8081b-975d-4db3-8a8f-2deed4ebf57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/mason/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "HF_TOKEN = \"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\"\n",
    "login(token=HF_TOKEN)\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d08c2-e12d-43c0-af24-d352db099355",
   "metadata": {},
   "source": [
    "### Upload a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7c5559-5314-4da5-b4a1-5bda69893e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mylesdyson/speecht5-tts-demo/commit/ab31e8f04d2b688d98b41ae90b43f63b2d2adee8', commit_message='Upload README.md with huggingface_hub', commit_description='', oid='ab31e8f04d2b688d98b41ae90b43f63b2d2adee8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"../../README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=\"mylesdyson/speecht5-tts-demo\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18041584-8d46-4ead-a6fa-ef25ed236a46",
   "metadata": {},
   "source": [
    "### Upload a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4664b603-beca-4567-8c61-7f0f82b075bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mylesdyson/speecht5-tts-demo/commit/6bbaae35b1f96c45f39987f494f3f378503d915e', commit_message='Upload folder using huggingface_hub', commit_description='', oid='6bbaae35b1f96c45f39987f494f3f378503d915e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    folder_path=\"../../training_data\",\n",
    "    repo_id=\"mylesdyson/speecht5-tts-demo\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a005a-44e0-4add-8ab8-aea309629687",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "By default, the .gitignore file will be taken into account to know which files should be committed or not. By default we check if a .gitignore file is present in a commit, and if not, we check if it exists on the Hub. Please be aware that only a .gitignore file present at the root of the directory with be used. We do not check for .gitignore files in subdirectories.\n",
    "\n",
    "If you don’t want to use an hardcoded .gitignore file, you can use the allow_patterns and ignore_patterns arguments to filter which files to upload. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as documented here. If both allow_patterns and ignore_patterns are provided, both constraints apply.\n",
    "\n",
    "Beside the .gitignore file and allow/ignore patterns, any .git/ folder present in any subdirectory will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e9a2751-03b7-428c-8830-11f3513fc262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mylesdyson/speecht5-tts-demo/commit/bb629d974aa835a4b42ea378ac1b79cc2e682373', commit_message='Upload folder using huggingface_hub', commit_description='', oid='bb629d974aa835a4b42ea378ac1b79cc2e682373', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    folder_path=\"../../training_data\",\n",
    "    path_in_repo=\"my-dataset/train\", # Upload to a specific folder\n",
    "    repo_id=\"mylesdyson/speecht5-tts-demo\",\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=\"**/logs/*.txt\", # Ignore all text logs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6f67c-ace0-4d94-babd-13b483bbd6bb",
   "metadata": {},
   "source": [
    "You can also use the delete_patterns argument to specify files you want to delete from the repo in the same commit. This can prove useful if you want to clean a remote folder before pushing files in it and you don’t know which files already exists.\n",
    "\n",
    "The example below uploads the local ./logs folder to the remote /experiment/logs/ folder. Only txt files are uploaded but before that, all previous logs on the repo on deleted. All of this in a single commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d59fbc4d-33fd-4a9a-ad83-f0f1c8cc6b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mylesdyson/speecht5-tts-demo/commit/5a7eacb6823c05f40bfef26de672860d6316aed4', commit_message='Upload folder using huggingface_hub', commit_description='', oid='5a7eacb6823c05f40bfef26de672860d6316aed4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    folder_path=\"../../training_data\",\n",
    "    path_in_repo=\"my-dataset/train\", # Upload to a specific folder\n",
    "    repo_id=\"mylesdyson/speecht5-tts-demo\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=\"*.txt\", # Upload all local text files\n",
    "    delete_patterns=\"*.txt\", # Delete all remote text files before\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b610b7-3185-4387-bc1f-470083360dee",
   "metadata": {},
   "source": [
    "## Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90490a4-8da1-4a0b-84b8-e82c2517a18a",
   "metadata": {},
   "source": [
    "### Non-blocking uploads\n",
    "This is particularly useful to upload logs and artifacts while continuing a training. To do so, you can use the run_as_future argument in both upload_file() and upload_folder(). This will return a concurrent.futures.Future object that you can use to check the status of the upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e428670-4b2a-4bc1-96cc-7f0957e470d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mylesdyson/speecht5-tts-demo/commit/13d72c4147cd7c977cdee5ec54900bd4fcc60ddf', commit_message='Upload folder using huggingface_hub', commit_description='', oid='13d72c4147cd7c977cdee5ec54900bd4fcc60ddf', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future = api.upload_folder( # Upload in the background (non-blocking action)\n",
    "    folder_path=\"../../training_data\",\n",
    "    path_in_repo=\"my-dataset/train\", # Upload to a specific folder\n",
    "    repo_id=\"mylesdyson/speecht5-tts-demo\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=\"*.txt\", # Upload all local text files\n",
    "    delete_patterns=\"*.txt\"\n",
    ")\n",
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a87e55-05e1-46b7-82d3-846f55e92af0",
   "metadata": {},
   "source": [
    "#### Background jobs\n",
    "Background jobs are queued when using run_as_future=True. This means that you are guaranteed that the jobs will be executed in the correct order.\n",
    "\n",
    "Even though background jobs are mostly useful to upload data/create commits, you can queue any method you like using run_as_future(). For instance, you can use it to create a repo and then upload data to it in the background. The built-in run_as_future argument in upload methods is just an alias around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e02f19c1-9d5e-452a-bc85-a1b2a3d1a49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Future at 0x7fbc3aa58430 state=pending>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future = api.upload_folder( # Upload in the background (non-blocking action)\n",
    "    folder_path=\"../../training_data\",\n",
    "    path_in_repo=\"my-dataset/train\", # Upload to a specific folder\n",
    "    repo_id=\"mylesdyson/speecht5-tts-demo\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=\"*.txt\", # Upload all local text files\n",
    "    delete_patterns=\"*.txt\",\n",
    "    run_as_future=True\n",
    ")\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a66aeab4-effa-4f2c-af76-dd762115cc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be3db492-2429-4068-9fac-6425f240d41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mylesdyson/speecht5-tts-demo/commit/4bced1abca2798ec7d4754dc714adb01776a28e6', commit_message='Upload folder using huggingface_hub', commit_description='', oid='4bced1abca2798ec7d4754dc714adb01776a28e6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ff89c-3429-4a43-88fd-70fbeb1f6c73",
   "metadata": {},
   "source": [
    "### Chunk upload\n",
    "To upload a folder in multiple commits, just pass multi_commits=True as argument. Under the hood, huggingface_hub will list the files to upload/delete and split them in several commits. The “strategy” (i.e. how to split the commits) is based on the number and size of the files to upload. A PR is open on the Hub to push all the commits. Once the PR is ready, the commits are squashed into a single commit. If the process is interrupted before completing, you can rerun your script to resume the upload. The created PR will be automatically detected and the upload will resume from where it stopped. It is recommended to pass multi_commits_verbose=True to get a better understanding of the upload and its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4a4db23-ef67-488e-8974-bf7f7916efce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Future at 0x7fbc3a3ff8e0 state=pending>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future = api.upload_folder( # Upload in the background (non-blocking action)\n",
    "    folder_path=\"../../training_data\",\n",
    "    path_in_repo=\"my-dataset/train\", # Upload to a specific folder\n",
    "    repo_id=\"mylesdyson/speecht5-tts-demo\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=\"*.txt\", # Upload all local text files\n",
    "    delete_patterns=\"*.txt\",\n",
    "    run_as_future=True,\n",
    "    multi_commits=True,\n",
    "    multi_commits_verbose=True\n",
    ")\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dd2145a-75ca-46cb-b6ba-396bd18e7489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0018254d-7d0c-4fa1-8da0-f82ab1cc3d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/mylesdyson/speecht5-tts-demo/discussions/2'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a77be-39e7-430d-a675-abe027010694",
   "metadata": {},
   "source": [
    "### Scheduled uploads\n",
    "The idea is to run a background job that regularly pushes a local folder to the Hub. Let’s assume you have a Gradio Space that takes as input some text and generates two translations of it. Then, the user can select their preferred translation. For each run, you want to save the input, output, and user preference to analyze the results. This is a perfect use case for CommitScheduler; you want to save data to the Hub (potentially millions of user feedback), but you don’t need to save in real-time each user’s input. Instead, you can save the data locally in a JSON file and upload it every 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1383b319-84bc-4dfa-a676-4409d13df345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gradio Jinja2\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "from huggingface_hub import CommitScheduler\n",
    "\n",
    "feedback_file = Path(\"../../training_data\") / f\"kjbible.txt\"\n",
    "feedback_folder = feedback_file.parent\n",
    "\n",
    "scheduler = CommitScheduler(\n",
    "    repo_id=\"mylesdyson/speecht5-tts-demo\",\n",
    "    repo_type=\"dataset\",\n",
    "    folder_path=feedback_folder,\n",
    "    path_in_repo=\"data\",\n",
    "    every=10,\n",
    ")\n",
    "\n",
    "def save_feedback(input_text:str, output_1: str, output_2:str, user_choice: int) -> None:\n",
    "    \"\"\"\n",
    "    Append input/outputs and user feedback to a JSON Lines file using a thread lock to avoid concurrent writes from different users.\n",
    "    \"\"\"\n",
    "    with scheduler.lock:\n",
    "        with feedback_file.open(\"a\") as f:\n",
    "            f.write(json.dumps({\"input\": input_text, \"output_1\": output_1, \"output_2\": output_2, \"user_choice\": user_choice}))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    ... # define Gradio demo + use `save_feedback`\n",
    "#demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc45375-5afb-465c-901a-3b8b61cc446e",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- append-only: It is assumed that you will only add content to the folder. You must only append data to existing files or create new files. Deleting or overwriting a file might corrupt your repository.\n",
    "- git history: The scheduler will commit the folder every every minutes. To avoid polluting the git repository too much, it is recommended to set a minimal value of 5 minutes. Besides, the scheduler is designed to avoid empty commits. If no new content is detected in the folder, the scheduled commit is dropped.\n",
    "- errors: The scheduler run as background thread. It is started when you instantiate the class and never stops. In particular, if an error occurs during the upload (example: connection issue), the scheduler will silently ignore it and retry at the next scheduled commit.\n",
    "- thread-safety: In most cases it is safe to assume that you can write to a file without having to worry about a lock file. The scheduler will not crash or be corrupted if you write content to the folder while it’s uploading. In practice, it is possible that concurrency issues happen for heavy-loaded apps. In this case, we advice to use the scheduler.lock lock to ensure thread-safety. The lock is blocked only when the scheduler scans the folder for changes, not when it uploads data. You can safely assume that it will not affect the user experience on your Space.\n",
    "\n",
    "#### Demo\n",
    "Persisting data from a Space to a Dataset on the Hub is the main use case for CommitScheduler. Depending on the use case, you might want to structure your data differently. The structure has to be robust to concurrent users and restarts which often implies generating UUIDs. Besides robustness, you should upload data in a format readable by the 🤗 Datasets library for later reuse. We created a Space that demonstrates how to save several different data formats (you may need to adapt it for your own specific needs).\n",
    "\n",
    "https://huggingface.co/spaces/Wauplin/space_to_dataset_saver\n",
    "\n",
    "### Custom uploads\n",
    "\n",
    "CommitScheduler assumes your data is append-only and should be uploading “as is”. However, you might want to customize the way data is uploaded. You can do that by creating a class inheriting from CommitScheduler and overwrite the push_to_hub method (feel free to overwrite it any way you want). You are guaranteed it will be called every every minutes in a background thread. You don’t have to worry about concurrency and errors but you must be careful about other aspects, such as pushing empty commits or duplicated data.\n",
    "\n",
    "In the (simplified) example below, we overwrite push_to_hub to zip all PNG files in a single archive to avoid overloading the repo on the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a538c9-3cfd-4fc1-a4c7-c265ae10f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZipScheduler(CommitScheduler):\n",
    "    def push_to_hub(self):\n",
    "        # 1. List PNG files\n",
    "          png_files = list(self.folder_path.glob(\"*.png\"))\n",
    "          if len(png_files) == 0:\n",
    "              return None  # return early if nothing to commit\n",
    "\n",
    "        # 2. Zip png files in a single archive\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            archive_path = Path(tmpdir) / \"train.zip\"\n",
    "            with zipfile.ZipFile(archive_path, \"w\", zipfile.ZIP_DEFLATED) as zip:\n",
    "                for png_file in png_files:\n",
    "                    zip.write(filename=png_file, arcname=png_file.name)\n",
    "\n",
    "            # 3. Upload archive\n",
    "            self.api.upload_file(..., path_or_fileobj=archive_path)\n",
    "\n",
    "        # 4. Delete local png files to avoid re-uploading them later\n",
    "        for png_file in png_files:\n",
    "            png_file.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
